{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "# download - скачать с сервера на диск\n",
    "test_data = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader - механизм, который на каждой итерации обучения нейросети будет выделять из датасета батч картинок и подавать сети на вход.\n",
    "Важно - перемешивать данные перед train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=len(test_data), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "# батч картинок и батч ответов к картинкам\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_imgs(imgs, labels):\n",
    "    f, axes = plt.subplots(1, 10, figsize=(30, 5))\n",
    "    for idx, axis in enumerate(axes):\n",
    "        axes[idx].imshow(np.squeeze(np.transpose(imgs[idx].numpy(), (1, 2, 0))), cmap='gray')\n",
    "        axes[idx].set_title(labels[idx].numpy())\n",
    "    plt.show()\n",
    "    \n",
    "show_imgs(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таблица соответствий номеров ответов и классов:\n",
    "\n",
    "| Label        | Class           |\n",
    "| ------------- |:-------------:|\n",
    "| 0     | Самолет |\n",
    "| 1    | Автомобиль      | \n",
    "| 2 | Птица      |\n",
    "| 3 | Кошка      |\n",
    "| 4 | Олень      |\n",
    "| 5 | Собака      |\n",
    "| 6 | Лягушка     |\n",
    "| 7 | Лошадь     |\n",
    "| 8 | Корабль     |\n",
    "| 9 | Грузовик     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем нужные модули для обучения сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# модуль, где определены слои для нейронных сетей\n",
    "import torch.nn as nn\n",
    "# модуль, где определены активации для слоев нейронных сетей\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение и тест базовой сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала построим двухслойную полносвязную нейронную сеть, обучим ее и посчитаем метрику accuracy на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# класс для удобного перевода картинки из двумерного объекта в вектор\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = nn.Linear(32*32*3, 256) # полносвязный слой в pytorch\n",
    "        self.fc2 = nn.Linear(256, 10) # классификация на 10 классов\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # forward pass сети\n",
    "        \n",
    "        # переводим входной объект из картинки в вектор\n",
    "        x= self.flatten(x)\n",
    "        # умножение на матрицу весов 1 слоя и применение функции активации\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # умножение на матрицу весов 2 слоя и применение функции активации\n",
    "        x = F.softmax(self.fc2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вынесем код обучения сети в отдельную функцию, чтобы каждый раз при изменении сети не копировать его. Код обучения мы менять не будем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, n_epoch=2):\n",
    "    # выбираем функцию потерь\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # выбираем алгоритм оптимизации и learning_rate\n",
    "    learning_rate = 1e-3\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # обучаем сеть 2 эпохи\n",
    "    for epoch in tqdm_notebook(range(n_epoch)):\n",
    "        running_loss = 0.0\n",
    "        train_dataiter = iter(train_loader)\n",
    "        for idx, batch in enumerate(tqdm_notebook(train_dataiter)):\n",
    "            # так получаем текущий батч\n",
    "            X_batch, y_batch = batch\n",
    "            \n",
    "            # обнуляем веса\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward pass (получение ответов на батч картинок)\n",
    "            y_pred = net(X_batch)\n",
    "            # вычисление потерь от выданных сетью ответов и правильных ответов на батч\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # backpropagation (вычисление градиентов)\n",
    "            loss.backward()\n",
    "            # обновление весов сети\n",
    "            optimizer.step()\n",
    "            \n",
    "            # выведем текущий loss\n",
    "            running_loss += loss.item()\n",
    "            # выведем качество каждые 500 батчей\n",
    "            if idx % 500 == 499:\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, idx + 1, running_loss / 500))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "    print('Обучение закончено')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объявляем сеть\n",
    "net = SimpleNet()\n",
    "net = train(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем accuracy на test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataiter = iter(test_loader)\n",
    "images, labels = test_dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(labels.numpy(), np.argmax(net.forward(images).detach().numpy(), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение сверточной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полносвязна нейронная сеть не очнь хорошо справляется с поставленной задачей. Попробуем объявить и обучить свеврточную нейронную сеть.\n",
    "\n",
    "Наша нейронная сеть будет содержать три слоя: два сверточных и один полносвязный. В качестве функции активации для внутренних слоев будем использовать relu, для последнего слоя - softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# класс для удобного перевода картинки из двумерного объекта в вектор\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 20, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(20, 3, kernel_size=3)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc = nn.Linear(2352, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # forward pass сети\n",
    "        # умножение на матрицу весов 1 слоя и применение функции активации\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        # умножение на матрицу весов 2 слоя и применение функии активации\n",
    "        x = F.softmax(self.fc(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataiter = iter(test_loader)\n",
    "images, labels = test_dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(labels.numpy(), np.argmax(net.forward(images).detach().numpy(), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BachNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одно из улучшений нейронной сети -- Батч Норм (BatchNorm).\n",
    "\n",
    "Смысл Батч Норма состоит в следующем: как вы знаете, когда мы обучаем нейронную сеть, мы обучаем ее батчами. Каждый батч картинок имеет разное распределение. Когда батч проходит через первый слой сети, выход первого слоя сети тоже имеет какое-то распределение. И для всех батчей оно немного разное.\n",
    "\n",
    "Таким образом, второй слой сети каждый раз принимает на вход данные, распределенные по-разному. И при обработке первого батча картинок веса сети подстраиваются под распределение картинок этого батча, а при обработке второго должны подстраиваться под другое распределение второго батча. Такое \"метание\" плохо сказывается на обучении сети, оно становится менее стабильным и в итоге сеть обучается хуже.\n",
    "\n",
    "Батч норм -- это специальный слой сети, который используется после каждого слоя нейронной сети (кроме последнего). Он нормализует выходы слоев. Таким образом, следуюшему слою нейронной сети каждый раз передаются данные, распределенные примерно одинаково.\n",
    "\n",
    "Чтобы лучше осознать полезность батч норма, можно вспомнить, что при изучении КНН мы говорили о том, что перед обучением модели признаки можно нормировать. Если есть два признака (например, возраст и доход), один из которых представлен десятками, а другой -- десятками тысяч, то для лучшего обучения модели их лучше привести к одному виду. Так и тут -- для лучшего обучения нейронной сети батчи лучше привести к одному виду.\n",
    "\n",
    "Более подробно прочитать про батч норм можно тут: http://neerc.ifmo.ru/wiki/index.php?title=Batch-normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс нейронной сети из трех слоев с батч нормализацией. За основу возьмем сеть, написанную выше: три слоя, 256, 64 и 10 нейронов в каждом.\n",
    "\n",
    "Задание: реализуйте код нейронной сети с батч нормами. Допишите код в ячейке ниже в недостающих местах и запустите обучение сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# класс для удобного перевода картинки из двумерного объекта в вектор\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = <первый линейный слой размерности <размер вектора картинки>*256>\n",
    "        # задаем слой батч норма для выхода слоя сети с 256 нейронами\n",
    "        self.bn1 = nn.BatchNorm1d(256) \n",
    "        self.fc2 = <первый линейный слой размерности 256*64>\n",
    "        self.bn2 = <второй батч норм для выхода слоя сети с 64 нейронами>\n",
    "        self.fc3 = <первый линейный слой размерности 64*10>\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward pass сети\n",
    "\n",
    "        # переводим входной объект из картинки в вектор\n",
    "        x = self.flatten(x)\n",
    "        # применение слоев и батч нормов по очереди:\n",
    "        <пропустите х через все слои и активации: fc1-relu-bn1-fc2-relu-bn2-fc3-softmax>\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объявляем сеть\n",
    "net = SimpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# не изменяйте эту ячейку!\n",
    "# эта ячейка проверяет ваш код на правильность\n",
    "# если при запуске ячейка выдает ошибку, то у вас в коде ошибка\n",
    "\n",
    "assert len(net.state_dict().keys()) == 16, \"Неверное количество слоев. \\\n",
    "Проверьте, что в вашей сети три слоя и два батч-норма после первого и второго слоя.\"\n",
    "\n",
    "fc1 = list(net.state_dict().keys())[0]\n",
    "fc2 = list(net.state_dict().keys())[7]\n",
    "fc3 = list(net.state_dict().keys())[14]\n",
    "\n",
    "assert net.state_dict()[fc1].shape == (256, 3072), \"Неверные размерности первого слоя. \\\n",
    "Должно быть размер вектора картинки * 256\"\n",
    "assert net.state_dict()[fc2].shape == (64, 256), \"Неверные размерности второго слоя. \\\n",
    "Должно быть 256 * 64\"\n",
    "assert net.state_dict()[fc3].shape == (10, 64), \"Неверные размерности третьего слоя. \\\n",
    "Должно быть 64 * 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataiter = iter(test_loader)\n",
    "images, labels = test_dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# убедиться, что accuracy увеличился! Batch Norm действительно работает.\n",
    "accuracy_score(labels.numpy(), np.argmax(net.forward(images).detach().numpy(), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наверное, вы уже заметили, что у нас в модели довольно много гиперпараметров. Например: количество слоев, размеры каждого слоя, learning rate в градиентном спуске. Как и в не нейронных моделях машинного обучения, эти гиперпараметры можно подбирать. Этим мы сейчас и займемся.\n",
    "\n",
    "Перенесем сюда код класса нейронной сети и функции обучения.\n",
    "\n",
    "Задание: проведите несколько экспериментов (минимум 3) обучения сети с разными параметрами. Варьируйте количество слоев, количество нейронов в слоях (у нас выше были 256 и 64, попробуйте других числа), различные функции активации после внутренних слоев (после последнего оставьте softmax) и разный learning rate. Опишите свои выводы на основе проделанных экспериментов в отдельной ячейке ниже.\n",
    "\n",
    "P.S. разные функции активации, как и relu, можно найти в torch.functional, который у нас импортирован как F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# примеры функций активации\n",
    "F.relu\n",
    "F.celu\n",
    "F.elu\n",
    "F.leaky_relu\n",
    "F.prelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описать выводы на основе проделанных экспериментов тут. Как повляили различные значения гиперпараметров на итоговую величиную accuracy? Как вы думаете, почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# класс для удобного перевода картинки из двумерного объекта в вектор\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = <первый линейный слой какой-нибудь размерности>\n",
    "        <еще слои, батч нормы на ваш выбор>\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward pass сети\n",
    "\n",
    "        # переводим входной объект из картинки в вектор\n",
    "        x = self.flatten(x)\n",
    "        <примените ваши слои, объявленные в init. Не забудьте про функици активации!>\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net):\n",
    "  # выбираем функцию потерь\n",
    "  loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "  # выбираем алгоритм оптимизации и learning_rate\n",
    "  learning_rate = <попробуйте обучить сеть с разными значениями learning rate. Например, 0.1, 0.01, ...>\n",
    "  optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "  # обучаем сеть 2 эпохи\n",
    "  for epoch in tqdm_notebook(range(2)):\n",
    "\n",
    "      running_loss = 0.0\n",
    "      train_dataiter = iter(train_loader)\n",
    "      for i, batch in enumerate(tqdm_notebook(train_dataiter)):\n",
    "          # так получаем текущий батч\n",
    "          X_batch, y_batch = batch\n",
    "          \n",
    "          # обнуляем веса\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          # forward pass (получение ответов на батч картинок)\n",
    "          y_pred = net(X_batch)\n",
    "          # вычисление лосса от выданных сетью ответов и правильных ответов на батч\n",
    "          loss = loss_fn(y_pred, y_batch)\n",
    "          # bsckpropagation (вычисление градиентов)\n",
    "          loss.backward()\n",
    "          # обновление весов сети\n",
    "          optimizer.step()\n",
    "\n",
    "          # выведем текущий loss\n",
    "          running_loss += loss.item()\n",
    "          # выведем качество каждые 500 батчей\n",
    "          if i % 500 == 499:\n",
    "              print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 500))\n",
    "              running_loss = 0.0\n",
    "\n",
    "  print('Обучение закончено')\n",
    "  return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleNN()\n",
    "net = train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataiter = iter(test_loader)\n",
    "images, labels = test_dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(labels.numpy(), np.argmax(net.forward(images).detach().numpy(), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это далеко не все способы улучшить сеть. Есть еще много. Например, Dropout (дропаут) помогает бороться с переобчением сетей. Почитать о нем можно тут: Дропаут на викиконспектах.\n",
    "\n",
    "Вообще, подбор параметров для обучения нейронной сети -- очень сложная задача. И чем задача сложнее (машинный перевод, генерация изображений и т.д.), тем нейронные сети становятся глубже и имеют все больше параметров. Для того, чтобы хорошо обучить сеть для сложной задачи, нужно подобрать архитектуру, количество слоев, количество нейронов в слое, правильный метод обучения -- learning rate, количество эпох и еще много других параметров. Это сделать весьма непросто."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
